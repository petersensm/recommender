# Recommender systems exercise 

This repository is a little exercise in playing with recommender systems and practicing R programming. I used  data from the [Million Song Dataset](http://labrosa.ee.columbia.edu/millionsong/), specifically the [Echo Nest Taste Profile subset](http://labrosa.ee.columbia.edu/millionsong/tasteprofile), and even more specifically a small portion of the subset of 120,000 users that one can access via the [Echo Nest](http://the.echonest.com/) API using a catalog of user id's. I also used the [recommenderlab package](https://cran.r-project.org/web/packages/recommenderlab/index.html) rather than writing my own recommender (which I'll save for some point in the future.)   

In this document, I outline the files in the repository and describe the flow and decisions for the function pipeline I wrote to access and process the data. You can follow along with the blog post scripts and the functions.   

# Files in the repository:  
* [blogpost.Rmd](https://github.com/petersensm/recommender/blob/master/blogpost.Rmd) --- is a narrative which describes the motivation and background for the project and leads the reader through a script that access the functions described in this document. The narrative concludes with an exploration of the results and discussion of future extensions. You can run the script to get recommendations for a subset of users via a simple collaborative filtering method and by a popular vote count, both methods build into recommenderlab.  
* [functions.R](https://github.com/petersensm/recommender/blob/master/functions.R) --- is the source file containing all the functions running in the script
* [tasteprofile600users.csv](https://github.com/petersensm/recommender/blob/master/tasteprofile600users.csv) --- is my pre-downloaded and cleaned taste profile dataset for 589 user ids, so you can run the script while bypassing the 30 minute step to gather this data from the [Echo Nest API](http://the.echonest.com/).

# Function pipeline
## Step i: Setup
### check_packages()    
Later functions depend on four packages being installed and loaded. Writing a function for this should have been trivial, but was not! The function I used was ultimately an adaptation of Shane's answer to a [Stackoverflow question](http://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them). check_packages() takes a character vector of the needed package names and checks for any that do not match the list of installed packages. Those that are missing are installed and then each package is loaded, one-by-one. Special thanks to Yihui Xie's blog post on [require() verses library()](http://yihui.name/en/2014/07/library-vs-require/) for dissuading me from my original approach using require()!    
## Step 1: Getting the data
The whole [Echo Nest Taste Profile subset](http://labrosa.ee.columbia.edu/millionsong/tasteprofile) is a huge dataset of information from over 1 million users. Instead, I used a subset of data from 120,000 users that one can access via the [Echo Nest](http://the.echonest.com/) API using a catalog of user id's. For more information on getting the data, follow the link above and scroll down for the info on the 120k user subset.

### get_catalog()  
#### Overview:
The get_catalog() function downloads the taste profile catalog using the provided url, saves the catalog in the working directory as "taste_profile_usercat_120k.txt", reads it into R, and retains entries where the catalog number is the correct size. This function takes no arguments.   

#### Details:   
The use of special characters, comments in embedded in rows, and some missing catalog numbers necessitated passing several arguments in read.table(). For example, comment characters were turned off because "#" starts each row, and the fill argument was set to TRUE to fill missing elements with blank spaces. Further, rather than use spaces as the delimiter, I used ">" because it precedes the catalog number and easily isolates it even when comments extending across other columns disrupt spacing. Despite this processing, there were a couple of instances were newlines were missing and catalog numbers were incorrectly parsed. Since all valid catalog numbers were 18 characters long, I used this criterion to subset the data --- eliminating rows with either missing catalog numbers or incorrect data in the catalog columns. In another situation, if I really needed all the data, I would work to clean up the entries where there were problems rather than eliminate them.   

### get_profile_data() or load_saved_data()
#### Overview:
The get_profile_data() function combines four accessory functions to retrieve user taste profile data from the Echo Nest API using three arguments: the catalog, a specified number of records to fetch, and the rate limit for calling the API. Note that to run the function, you'll need to register for an API key (see details below). Alternatively, you can simply call the function load_saved_data() to access the dataset I previously downloaded and posted to the repository. This is probably a good idea as downloading about 600 records at the default rate limit will take 30 minutes.  

The final output of the function is a data frame of user playlists containing six variables (not in this order): the user id, song id, song name, artist id, the artist name, the play count (i.e. how many times the song was played by the user). Using the default settings results in taste profile dataset for 589 users that is 8733 rows long. Information from 11 of the users was lost when removing awkward catalog entries and incomplete taste profile data. Again, you can see all of this in the previously saved data.

#### Details:
get_profile_data() uses get_catalog_ids() to make a data frame of user catalog ids by pseudo-randomly sampling the catalog for the number of records specified. I chose to work with 600 records, but you could choose more or less. Note: To get use the same set of catalog ids as in the data accompanying the repository, call set.seed(1) immediately before the main function to use the same random number seed as I used. Then, for each user play count catalog ID, the system is set to pause for a length that will maintain the rate limit, generate a URL, fetch the data, and process it. The default rate limit specified by Echo Nest API is 20 calls per minute, which makes the record  gathering process rather slow. Given this constraint, I decided just to gather a handful of records (600) since I just want to play a bit with recommender systems. However, if you provide additional information to Echo Nest, they will increase your rate limit to 120.

The URL is generated using the makeURL() function which concatenates the URL for accessing the taste profile data, the API key (see above), and the user catalog id. I've kept my API key hidden, so to run the function, you'll need to register for your own by following the [instructions at The Echo Nest](https://developer.echonest.com/account/register). Put your API key in a file called "api_key.R" containing the line, apikey <- "your API key here", but with your actual API key, of course. 

Retrieving the taste profile data content is achieved via the getusercontent() function which passes the URL to GET() from the httr package and extracts the content from the JSON format using fromJSON() in the jsonlite package. This is fairly simple (once you know what you're doing), as no oath authentication is needed, just the read command from the taste profile methods, the API key, and the user catalog ID. See Echo Nest's [resources for developers](http://developer.echonest.com/) for more general information and the [read command in the taste profile methods](http://developer.echonest.com/docs/v4/taste profile.html#read) for specifics. If all the elements are present, the relevant information of the user's playlist is then extracted to a data frame and concatenated to the previous user's record.  

## Step 2: Process the data, feed to recommenderlab's functions, and extract song predictions
I put all of this in one function for convenience. But an alternative would be to run the data processing steps separately and then following along with the recommenderlab vignette instead of using the rest of the wrapper functions. This is especially ideal if you want try comparing the performance of different recommenders.  

### get_recommendations()
#### Overview: 
The get_recommendations() function uses a handful of accessory functions to funnel the taste profile data into and out of the functions of the recommenderlab package. These recommenderlab functions do the heavy lifting of transforming the dataframe into a binary (presence/absence) matrix, applying a simple recommender system, and generating recommendations. The arguments passed to get_recommendations() include the taste profile dataset, your choice of recommender system method, the percentage of data to be used for "training" the recommender (versus testing or retrieving recommendations), and the number of recommendations offered to each user in the test data. For recommender systems, one can choose either the user-based collaborative filtering method using the Jaccard similarity ("UBCF") or a super simple popular count ("POPULAR"), which is an interesting method for comparison. The final output is a list of lists, one  for each user in the test dataset, containing their id number, playlist (as a data frame), and top recommendations (also a data frame). The length of the main list is long as the number of users in the test data.   

#### Details:
**Processing the data**   
The get_recommendations() function prepares and processes the data using the process_data() function which reduces the taste profile data to tupels of user, item, and play count. Then, missing play count data (value = NA) are converted to 1, with the assumption that if the user added it to their playlist, this indicates some preference for the song. For the data set I worked with, 11% of the play counts were NA's. Then, the recommenderlab functions as() and binarize() are used to convert the data frame into a realratingsmatrix class object (a user x item matrix of play counts) and then into a binaryratingmatrix class object in which all unrated songs are given a value of 0 and all rated songs are given a value of one. These object classes are internal to  recommenderlab, so now the dataset is ready for feeding into subsequent recommenderlab functions.   

**Splitting the data**   
Next, the data in the binary rating matrix are indexed for splitting into a training dataset and testing a dataset. The users in the training dataset are used to model the recommender system and the users in the test dataset are provided recommendations. In this application, the distinction between a training and test dataset is not as important as when one is actually evaluating the effectiveness of a recommender. In that case, rating (here, play count) data would be removed at random from the test data so that observed and predicted valued could be compared. The default is a 90% training, 10% test split. The split is achieved by pseudo-randomly sampling the row index of the matrix, and the index is output from the function where it can be used to select the appropriate rows, or their inverse, as needed. I originally output either the training or testing dataset, but realized that in calling the function twice, I was accidentally re-sampling the data rather than splitting it. The current function is modeled after the one employed in the recommenderlab package.    

**Feeding into the recommender of choice**   
The training data is then passed into recommenderlab's Recommender() function. For the user-based collaborative filtering method "UBCF", the Jaccard similarity is calculated for all user pairs. Then, when the prediction function is called, recommendations for a given user are based on ranking of the average rating given to a song by the 5 users that are most similar to the focal user (i.e. the 5 nearest neighbors). One could use a different number of nearest neighbors, but I arbitrarily chose 5. For the "POPULAR" method, recommendations are simply based on the most popular songs. This method uses column sums of items to identify the most popular items, and orders items by decreasing value. Predictions are then made by providing the top most popular items not currently in the focal user's playlist.  

**Making predictions**   
Making predictions involves calling recommenderlab's prediction() function on the output of the Recommender() function which contains instructions for making the top recommendations for users in the test data. In the case of the "POPULAR" recommender, it uses the popular songs from the training data. For the "UBCF" method, the training and test are both used for calculating similarity and identifying nearest neighbors to make recommendations. Since we are using a binary matrix, the prediction() function simply return a list of lists - with top recommended songs for each user. The number of recommendations returned is 5 by default (in my deployment of the function), but can be changed by passing a different value to the n = argument in the get_recommendations() function.   

**Processing the recommendations and compiling the output**   
The final set of functions are contained in the process_recommendations() function which takes the lists of recommended song ids and puts them in a digestible context. Process_recommendations() ultimately generates a list for each of the users in the test data. The first item in the list is the user id, which is fetched from the test data by get_user_id(). The second item in the list is the user's playlist, consisting of song id, song name, artist id, and artist name, which are presented in data frame format and fetched from the taste profile data by get_user_playlist(). The third and final item in the list is a data frame of the top recommended songs consisting of the the same elements as the playlist which are fetched from both the predictions() output and an index of songs in the taste profile data by get_user_recs(). There are too concerns with this final function: one, the song index is made using the dplyr package, which I love using interactively, but would prefer not to call inside a function; two, the artist matching in the song index could be off in a small number of cases as there are six songs in the dataset I used that have different artists associated with the same song ID.   
